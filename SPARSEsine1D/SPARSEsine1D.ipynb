{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danidmvz/SPARSEwithRL/blob/main/SPARSEsine1D/SPARSEsine1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CYYsHV8_OYv"
      },
      "source": [
        "# **SPARSE with stochastic forcing: what is the optimal forcing function?** \n",
        "\n",
        "In this code I present a one-dimensional version of the SPARSE method with stochastic forcing (S-SPARSE) to describe the agent that is trying to minimize some Quantity of Interest (QoI) as it might be the particle spread (standard deviation of the particle positions) $\\sigma_{x_p}$.\n",
        "\n",
        "The agent: macro-particle, learns the stochastic forcing function $f(\\boldsymbol \\alpha,\\boldsymbol a)=\\sum_i^{N}\\alpha_i \\Psi_i(\\boldsymbol a)$ and in particular its coefficients stuck in the vector $\\boldsymbol \\alpha$. The basis functions $\\Psi_i$ are chosen according to the Chebyshev modes of the first kind of a correlation. In this case I use the Schiller and Naumann correlation $f(Re_p)=1+0.15Re_p^{0.687}$.\n",
        "\n",
        "The agent learns to control the second moments of the stochastic coefficients $\\overline{\\alpha_i^\\prime \\alpha_j^\\prime}$ to minimize the cost function that is defined such that the particle spread is minimum. The agent is trained using maximum a posteriori policy optimization ([mpo](https://arxiv.org/abs/1806.06920)). The deep reinforcment learning framework [acme](https://github.com/deepmind/acme), developed by DeepMind, is used to train the agent.\n",
        "\n",
        "The notebook describes in order: \n",
        "- The environment components and  the implementation architecture required by [dm_control](https://github.com/deepmind/dm_control).\n",
        "- The agent set-up and training loop. \n",
        "- The evaluation of the trained agent when applied to the environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXq_KfoWtfi"
      },
      "source": [
        "# Install acme\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install acme\n",
        "!pip install virtualenv\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!virtualenv .acme\n",
        "!source .acme/bin/activate\n",
        "\n",
        "# A fixed release version of dm-acme and dm-control is enforced for long term maintenance\n",
        "!pip install dm-acme[tf]==0.3.0 \n",
        "!pip install dm-control==0.0.364896371"
      ],
      "metadata": {
        "id": "5yFhnT4HYDVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ea0247-18ce-48b4-e79f-6b07bb94f521"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.19.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.4.1 in /usr/local/lib/python3.8/dist-packages (from virtualenv) (3.9.0)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv) (2.6.2)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.6 virtualenv-20.19.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-67.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (0.38.4)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.0 setuptools-67.2.0\n",
            "created virtual environment CPython3.8.10.final.0-64 in 989ms\n",
            "  creator CPython3Posix(dest=/content/.acme, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.0, setuptools==67.1.0, wheel==0.38.4\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-acme[tf]==0.3.0\n",
            "  Downloading dm-acme-0.3.0.tar.gz (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from dm-acme[tf]==0.3.0) (1.4.0)\n",
            "Collecting dm-env\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Collecting dm-launchpad==0.4.1\n",
            "  Downloading dm_launchpad-0.4.1-cp38-cp38-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from dm-acme[tf]==0.3.0) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from dm-acme[tf]==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from dm-acme[tf]==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from dm-acme[tf]==0.3.0) (4.4.0)\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trfl\n",
            "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb==0.6.1\n",
            "  Downloading dm_reverb-0.6.1-cp38-cp38-manylinux2010_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-datasets==4.4.0\n",
            "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==2.7.0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_probability==0.15.0\n",
            "  Downloading tensorflow_probability-0.15.0-py2.py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (3.19.6)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (2.2.1)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (1.3.9)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (1.51.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (2.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from dm-launchpad==0.4.1->dm-acme[tf]==0.3.0) (5.4.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.12)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.14.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.38.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->dm-acme[tf]==0.3.0) (15.0.6.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (5.10.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (0.3.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (2.25.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (22.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.15.0->dm-acme[tf]==0.3.0) (4.4.2)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->dm-acme[tf]==0.3.0) (0.8.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (67.2.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (3.12.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets==4.4.0->dm-acme[tf]==0.3.0) (1.58.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (6.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->dm-acme[tf]==0.3.0) (3.2.2)\n",
            "Building wheels for collected packages: dm-acme\n",
            "  Building wheel for dm-acme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dm-acme: filename=dm_acme-0.3.0-py3-none-any.whl size=561383 sha256=ef1a95ef9e70c9dae0c0cda9405a13ea3a7e0496406642d20a740d89e562f29c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/d5/68/96139e6e314d8fe12a381976cf631fc1a5b9c546ab00529728\n",
            "Successfully built dm-acme\n",
            "Installing collected packages: tensorflow-estimator, keras, trfl, tensorflow_probability, mock, dm-sonnet, dm-reverb, dm-env, dm-launchpad, tensorflow-datasets, dm-acme, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow_probability\n",
            "    Found existing installation: tensorflow-probability 0.17.0\n",
            "    Uninstalling tensorflow-probability-0.17.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.17.0\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.8.2\n",
            "    Uninstalling tensorflow-datasets-4.8.2:\n",
            "      Successfully uninstalled tensorflow-datasets-4.8.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed dm-acme-0.3.0 dm-env-1.6 dm-launchpad-0.4.1 dm-reverb-0.6.1 dm-sonnet-2.0.1 keras-2.7.0 mock-5.0.1 tensorflow-2.7.0 tensorflow-datasets-4.4.0 tensorflow-estimator-2.7.0 tensorflow_probability-0.15.0 trfl-1.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-control==0.0.364896371\n",
            "  Downloading dm_control-0.0.364896371-py3-none-any.whl (18.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (1.4.0)\n",
            "Requirement already satisfied: dm-env in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (1.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (4.9.2)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (0.1.8)\n",
            "Requirement already satisfied: protobuf>=3.15.6 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (3.19.6)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (3.1.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (0.16.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (3.0.9)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (67.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (1.21.6)\n",
            "Collecting labmaze\n",
            "  Downloading labmaze-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (4.64.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from dm-control==0.0.364896371) (2.25.1)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.6-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->dm-control==0.0.364896371) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->dm-control==0.0.364896371) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->dm-control==0.0.364896371) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->dm-control==0.0.364896371) (2022.12.7)\n",
            "Installing collected packages: glfw, labmaze, dm-control\n",
            "Successfully installed dm-control-0.0.364896371 glfw-2.5.6 labmaze-1.0.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmyBOr3T5A-a"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from dm_control.rl.control import Environment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from acme import agents, specs\n",
        "from acme.environment_loop import EnvironmentLoop\n",
        "from acme.wrappers.single_precision import SinglePrecisionWrapper\n",
        "from acme.wrappers.canonical_spec import CanonicalSpecWrapper\n",
        "from acme.agents.tf import mpo\n",
        "from acme.tf import networks\n",
        "from acme.tf import utils as tf2_utils\n",
        "import sonnet as snt\n",
        "from acme.utils import paths\n",
        "import tensorflow as tf \n",
        "from acme.utils.loggers import tf_summary\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from dm_control.rl import control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4IpHUcX5NnL"
      },
      "source": [
        "#  **Water tank environment**\n",
        "\n",
        "## Physics model\n",
        "The ordinary differential equation governing the evolution of the water level $h$ in the tank, provided the appropriate physical scaling, can be written as,\n",
        "\\begin{align}\n",
        "\\frac{d h}{d t} = w_{out} + w_{in}.\n",
        "\\end{align}\n",
        "\n",
        "The following constitutive equation is assumed for the water outflow in the nozzle,\n",
        "\\begin{align}\n",
        "w_{out} = - \\alpha \\sqrt{h}.\n",
        "\\end{align}\n",
        "\n",
        "The equation is discretized in time with Euler explicit scheme,\n",
        "\\begin{align}\n",
        "h^{t+1} = dt_{sim}*(w_{out}^{t} + w_{in}^{t}) + h^{t}  \n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRyQish2m8Uq"
      },
      "source": [
        "## Environment in dm_control framework\n",
        "\n",
        "To make use of [acme](https://github.com/deepmind/acme) architecture for continuous control purposes it's convenient to implement the environment following the [dm_control](https://github.com/deepmind/dm_control) architecture. This allows to benefit from several tools and routines to simplify the set-up of the training.\n",
        "\n",
        "The [dm_control](https://github.com/deepmind/dm_control) framework requires to define the environment as a combination of a **physics** simular and one or multiple **tasks**. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN9pnDUXT9xt"
      },
      "source": [
        "### physics\n",
        "The main component of the **physics** simulator, given the actions, is to step in time the ode. This is done in the `step` method.\n",
        "\n",
        "Numerical checks for the solution as well as sanity checks to avoid non physical state of the system are included in the `check_divergence` method.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWSX8DV8mcZL"
      },
      "outputs": [],
      "source": [
        "class Physics(control.Physics):\n",
        "    \"\"\"Water tank environment built on the dm_control.Environment class.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dt: float,\n",
        "        init_state: float,\n",
        "        init_action: float,\n",
        "        min_Rep: float,\n",
        "        max_Rep: float,\n",
        "        Re_inf: float,\n",
        "        dp: float,\n",
        "        St: float,\n",
        "        ):\n",
        "        \"\"\"Initializes water tank\n",
        "\n",
        "        Attributes:\n",
        "            alpha: nozzle outflow coefficient\n",
        "            dt:  [s] Discretization time interval for sim\n",
        "            hmax: [m] max water height in tank\n",
        "            init_state: [m] initial water height        \n",
        "        \"\"\"\n",
        "        self._dt = dt\n",
        "        self._init_state = init_state\n",
        "        self._state = np.asarray(self._init_state)\n",
        "        self._time = 0.\n",
        "        self._init_action = init_action #  np.asarray([0. ,0.])\n",
        "        self._action = np.asarray(self._init_action)\n",
        "        self._min_Rep = min_Rep\n",
        "        self._max_Rep = max_Rep\n",
        "        self._Re_inf = Re_inf\n",
        "        self._dp = dp\n",
        "        self._St = St\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets environment physics\"\"\"\n",
        "        self._state = self._init_state\n",
        "        self._time = 0.\n",
        "        self._action = self._init_action #  np.zeros((2,1)) # np.asarray([0., 0.])\n",
        "\n",
        "\n",
        "    def after_reset(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def step(self, n_sub_steps = 1):\n",
        "        \"\"\"Updates the environment according to the action\"\"\"\n",
        "\n",
        "        # Euler explicit time step\n",
        "        #print(self._dt*self._RHS())\n",
        "        self._state =  self._state + self._dt*self._RHS()\n",
        "        #print(self._state)\n",
        "\n",
        "        # Update sim time\n",
        "        self._time += self._dt\n",
        "\n",
        "        # Keep h min at 0\n",
        "        # if self._state[0] <= 0.: self._state[0] = 0.\n",
        "\n",
        "\n",
        "    def _RHS(self):\n",
        "        \"\"\" Returns Physical RHS for ODE d state / dt = RHS(state, action) \"\"\"\n",
        "        # State variables\n",
        "        mean_xp = self._state[0]\n",
        "        mean_up = self._state[1]\n",
        "        prime_xpxp = self._state[2]\n",
        "        prime_xpup = self._state[3]\n",
        "        prime_upup = self._state[4]\n",
        "        prime_alpha1xp = self._state[5]\n",
        "        prime_alpha2xp = self._state[6]\n",
        "        prime_alpha3xp = self._state[7]\n",
        "        prime_alpha4xp = self._state[8]\n",
        "        prime_alpha5xp = self._state[9]\n",
        "        prime_alpha1up = self._state[10]\n",
        "        prime_alpha2up = self._state[11]\n",
        "        prime_alpha3up = self._state[12]\n",
        "        prime_alpha4up = self._state[13]\n",
        "        prime_alpha5up = self._state[14]\n",
        "\n",
        "        # Action variables \n",
        "        prime_alpha1alpha1 = self._action[0]\n",
        "        prime_alpha1alpha2 = self._action[1]\n",
        "        prime_alpha1alpha3 = self._action[2]\n",
        "        prime_alpha1alpha4 = self._action[3]\n",
        "        prime_alpha1alpha5 = self._action[4]\n",
        "        prime_alpha2alpha2 = self._action[5]\n",
        "        prime_alpha2alpha3 = self._action[6]\n",
        "        prime_alpha2alpha4 = self._action[7]\n",
        "        prime_alpha2alpha5 = self._action[8]\n",
        "        prime_alpha3alpha3 = self._action[9]\n",
        "        prime_alpha3alpha4 = self._action[10]\n",
        "        prime_alpha3alpha5 = self._action[11]\n",
        "        prime_alpha4alpha4 = self._action[12]\n",
        "        prime_alpha4alpha5 = self._action[13]\n",
        "        prime_alpha5alpha5 = self._action[14]\n",
        "\n",
        "        # Flow\n",
        "        u_at_mean_xp, dudx_at_mean_xp, d2udx_at_mean_xp = self._Flow()\n",
        "        mean_u = u_at_mean_xp + 0.5*( prime_xpxp*d2udx_at_mean_xp )\n",
        "        #print('flow',u_at_mean_xp)\n",
        "\n",
        "        # Terms xpu\n",
        "        prime_xpu = prime_xpxp*dudx_at_mean_xp\n",
        "\n",
        "        # Terms upu\n",
        "        prime_upu = prime_xpup*dudx_at_mean_xp\n",
        "\n",
        "        # Terms uu\n",
        "        prime_uu = prime_xpu*dudx_at_mean_xp\n",
        "\n",
        "        # Terms u with coefficients\n",
        "        prime_alpha1u = prime_alpha1xp*dudx_at_mean_xp;\n",
        "        prime_alpha2u = prime_alpha2xp*dudx_at_mean_xp;\n",
        "        prime_alpha3u = prime_alpha3xp*dudx_at_mean_xp;\n",
        "        prime_alpha4u = prime_alpha4xp*dudx_at_mean_xp;\n",
        "        prime_alpha5u = prime_alpha5xp*dudx_at_mean_xp;\n",
        "\n",
        "        # Terms with relative velocity\n",
        "        mean_ax = mean_u - mean_up\n",
        "        prime_alpha1ax = prime_alpha1u - prime_alpha1up;\n",
        "        prime_alpha2ax = prime_alpha2u - prime_alpha2up;\n",
        "        prime_alpha3ax = prime_alpha3u - prime_alpha3up;\n",
        "        prime_alpha4ax = prime_alpha4u - prime_alpha4up;\n",
        "        prime_alpha5ax = prime_alpha5u - prime_alpha5up;\n",
        "\n",
        "        # Forcing\n",
        "        f_at_means = self._Forcing(mean_ax)\n",
        "        mean_f = 1 # f_at_means + 0.5*( 1 ) # This is wrong\n",
        "\n",
        "        # Forcing with particle position\n",
        "        prime_xpf = prime_alpha1xp*0 # This is wrong\n",
        "          \n",
        "        # Forcing with particle velocity \n",
        "        prime_upf = prime_alpha1up*0 # This is wrong\n",
        "\n",
        "        # Forcing with flow\n",
        "        prime_uf = prime_xpf*dudx_at_mean_xp\n",
        "\n",
        "        # Forcing with coefficients\n",
        "        prime_alpha1f = prime_alpha2alpha2 # This is wrong\n",
        "        prime_alpha2f = prime_alpha2alpha2 # This is wrong\n",
        "        prime_alpha3f = prime_alpha3alpha3 # This is wrong\n",
        "        prime_alpha4f = prime_alpha4alpha4 # This is wrong\n",
        "        prime_alpha5f = prime_alpha5alpha5 # This is wrong\n",
        "\n",
        "\n",
        "        # Printing\n",
        "        #print('St',type(self._St))\n",
        "        #print('mean_xp', type(mean_xp))\n",
        "        #print('mean_up', type(mean_up))\n",
        "        #print('prime_xpxp', type(prime_xpxp))\n",
        "        #print('prime_xpup', type(prime_xpup))\n",
        "        #print('prime_upup', type(prime_upup))\n",
        "        #print('prime_alpha1xp', type(prime_alpha1xp))\n",
        "        #print('prime_alpha2xp', type(prime_alpha2xp))\n",
        "        #print('prime_alpha3xp', type(prime_alpha3xp))\n",
        "        #print('prime_alpha4xp', type(prime_alpha4xp))\n",
        "        #print('prime_alpha5xp', type(prime_alpha5xp))\n",
        "        #print('prime_alpha1up', type(prime_alpha1up))\n",
        "        #print('prime_alpha2up', type(prime_alpha2up))\n",
        "        #print('prime_alpha3up', type(prime_alpha3up))\n",
        "        #print('prime_alpha4up', type(prime_alpha4up))\n",
        "        #print('prime_alpha5up', type(prime_alpha5up))\n",
        "\n",
        "        # Equations\n",
        "        ddt_mean_xp =  1.*mean_up \n",
        "        #print('mean_up')\n",
        "        #print( (1./self._St)*( mean_f*(mean_u-mean_up) + prime_uf - prime_upf ) )\n",
        "        ddt_mean_up =  (1./self._St)*( mean_f*(mean_u-mean_up) + prime_uf - prime_upf ) \n",
        "        ddt_prime_xpxp =  2.*prime_xpup \n",
        "        ddt_prime_xpup =  prime_upup + ( mean_f*(prime_xpu-prime_xpup) + prime_xpf*(mean_u-mean_up) )/self._St \n",
        "        ddt_prime_upup = (  mean_f*(2.*prime_upu-2.*prime_upup) + 2.*prime_upf*(mean_u-mean_up) )*(1./self._St) \n",
        "        ddt_prime_alpha1xp =  prime_alpha1up \n",
        "        ddt_prime_alpha2xp =  prime_alpha2up \n",
        "        ddt_prime_alpha3xp =  prime_alpha3up \n",
        "        ddt_prime_alpha4xp =  prime_alpha4up \n",
        "        ddt_prime_alpha5xp =  prime_alpha5up \n",
        "        ddt_prime_alpha1up =  ( mean_f*(prime_alpha1u-prime_alpha1up) + prime_alpha1f*(mean_u-mean_up) )/self._St \n",
        "        ddt_prime_alpha2up =  ( mean_f*(prime_alpha2u-prime_alpha2up) + prime_alpha2f*(mean_u-mean_up) )/self._St \n",
        "        ddt_prime_alpha3up =  ( mean_f*(prime_alpha3u-prime_alpha3up) + prime_alpha3f*(mean_u-mean_up) )/self._St \n",
        "        ddt_prime_alpha4up =  ( mean_f*(prime_alpha4u-prime_alpha4up) + prime_alpha4f*(mean_u-mean_up) )/self._St \n",
        "        ddt_prime_alpha5up =  ( mean_f*(prime_alpha5u-prime_alpha5up) + prime_alpha5f*(mean_u-mean_up) )/self._St \n",
        "        \n",
        "        #print('ddt_mean_xp', type(ddt_mean_xp))\n",
        "        #print('ddt_mean_up', type(ddt_mean_up))\n",
        "        #print('ddt_prime_xpxp', type(ddt_prime_xpxp))\n",
        "        #print('ddt_prime_xpup', type(ddt_prime_xpup))\n",
        "        #print('ddt_prime_upup', type(ddt_prime_upup))\n",
        "        #print('ddt_prime_alpha1xp', type(ddt_prime_alpha1xp))\n",
        "        #print('ddt_prime_alpha2xp', type(ddt_prime_alpha2xp))\n",
        "        #print('ddt_prime_alpha3xp', type(ddt_prime_alpha3xp))\n",
        "        #print('ddt_prime_alpha4xp', type(ddt_prime_alpha4xp))\n",
        "        #print('ddt_prime_alpha5xp', type(ddt_prime_alpha5xp))\n",
        "        #print('ddt_prime_alpha1up', type(ddt_prime_alpha1up))\n",
        "        #print('ddt_prime_alpha2up', type(ddt_prime_alpha2up))\n",
        "        #print('ddt_prime_alpha3up', type(ddt_prime_alpha3up))\n",
        "        #print('ddt_prime_alpha4up', type(ddt_prime_alpha4up))\n",
        "        #print('ddt_prime_alpha5up', type(ddt_prime_alpha5up))\n",
        "\n",
        "        rhs = np.asarray([ddt_mean_xp, ddt_mean_up, ddt_prime_xpxp, ddt_prime_xpup, ddt_prime_upup, \n",
        "               ddt_prime_alpha1xp, ddt_prime_alpha2xp, ddt_prime_alpha3xp, ddt_prime_alpha4xp, ddt_prime_alpha5xp,\n",
        "               ddt_prime_alpha1up, ddt_prime_alpha2up, ddt_prime_alpha3up, ddt_prime_alpha4up, ddt_prime_alpha5up])\n",
        "        #print(rhs)\n",
        "        #print(type(rhs))\n",
        "        return rhs\n",
        "        # return np.append(ddt_mean_xp, ddt_mean_xp, ddt_prime_xpxp, ddt_prime_xpup, ddt_prime_upup, \n",
        "                         # ddt_prime_alphaxp1, ddt_prime_alphaxp2, ddt_prime_alphaxp3, ddt_prime_alphaxp4, ddt_prime_alphaxp5,\n",
        "                         # ddt_prime_alphaup1, ddt_prime_alphaup2, ddt_prime_alphaup3, ddt_prime_alphaup4, ddt_prime_alphaup5)\n",
        "        \n",
        "\n",
        "    def _Flow(self):\n",
        "      u0 = 1\n",
        "      A = 0.5\n",
        "      k = 2\n",
        "      w = 0\n",
        "      phi = 0 \n",
        "      t = self._time\n",
        "      x = self._state[0]\n",
        "      u = u0 + A*np.sin(phi - t*w + k*x)\n",
        "      dudx = A*k*np.cos(phi - t*w + k*x)\n",
        "      d2udx = -A*k**2*np.sin(phi -t*w + k*x)\n",
        "      return float(u), float(dudx), float(d2udx)\n",
        "\n",
        "\n",
        "    def _Forcing(self, mean_ax):\n",
        "        # Action variables \n",
        "        prime_alpha1alpha1 = self._action[0]\n",
        "        prime_alpha1alpha2 = self._action[1]\n",
        "        prime_alpha1alpha3 = self._action[2]\n",
        "        prime_alpha1alpha4 = self._action[3]\n",
        "        prime_alpha1alpha5 = self._action[4]\n",
        "        prime_alpha2alpha2 = self._action[5]\n",
        "        prime_alpha2alpha3 = self._action[6]\n",
        "        prime_alpha2alpha4 = self._action[7]\n",
        "        prime_alpha2alpha5 = self._action[8]\n",
        "        prime_alpha3alpha3 = self._action[9]\n",
        "        prime_alpha3alpha4 = self._action[10]\n",
        "        prime_alpha3alpha5 = self._action[11]\n",
        "        prime_alpha4alpha4 = self._action[12]\n",
        "        prime_alpha4alpha5 = self._action[13]\n",
        "        prime_alpha5alpha5 = self._action[14]\n",
        "        #print(self._Re_inf)\n",
        "        #print(mean_ax)\n",
        "        #print(self._dp)\n",
        "        \n",
        "        Rep = self._Re_inf*np.asarray(np.absolute(mean_ax), dtype=float)*self._dp\n",
        "        #a = self._min_Rep\n",
        "        #b = self._max_Rep\n",
        "        #n = 5\n",
        "        #N = n+1\n",
        "        #i = np.linspace(1, N, num=N)\n",
        "        #u_nodes = np.cos((2*i-1)*np.pi/(2*N))\n",
        "        #x_nodes = 0.5*(b - a)*u_nodes + 0.5*(b+a);\n",
        "        #y_nodes = 1 + 0.15*x_nodes**0.687;\n",
        "\n",
        "        #A = np.zeros((n,))\n",
        "        #A[0] = 1\n",
        "        #A[1:] = 2\n",
        "\n",
        "        #Tn1 = 1 + 0*u_nodes\n",
        "        #Tn2 = u_nodes\n",
        "        #Tn3 = 2*u_nodes**2 - 1\n",
        "        #Tn4 = 4*u_nodes**3 - 3*u_nodes\n",
        "        #Tn5 = 8*u_nodes**4 - 8*u_nodes**2 + 1\n",
        "\n",
        "        #c1 = A[1]*np.average(Tn1*y_nodes)\n",
        "        #c2 = A[2]*np.average(Tn2*y_nodes)\n",
        "        #c3 = A[3]*np.average(Tn3*y_nodes)\n",
        "        #c4 = A[4]*np.average(Tn4*y_nodes)\n",
        "        #c5 = A[5]*np.average(Tn5*y_nodes)\n",
        "\n",
        "        # For now this works for one macro-particle, I think we need a loop to \n",
        "        # extend it to several\n",
        "\n",
        "        return 1. # + 0.15*Rep**0.687 + prime_alpha1alpha1\n",
        "\n",
        "\n",
        "    def time(self):\n",
        "        \"\"\"Returns total elapsed simulation time\"\"\"\n",
        "        return self._time\n",
        "\n",
        "\n",
        "    def timestep(self):\n",
        "        \"\"\"Returns dt simulation step\"\"\"\n",
        "        return self._dt\n",
        "\n",
        "\n",
        "    def check_divergence(self):\n",
        "        \"\"\" Checks physical terminations:\n",
        "         - water reached maximum level\n",
        "         - physical states not finite\n",
        "         \"\"\"\n",
        "        #if  self._state[0] >= self_hmax:\n",
        "        #    raise control.PhysicsError(\n",
        "        #        f'h > max value = {self._hmax} [m]'\n",
        "        #    )\n",
        "        #if not all(np.isfinite(self._state)):\n",
        "        #    raise control.PhysicsError('System state not finite')\n",
        "        pass\n",
        "\n",
        "    def set_control(self, action):\n",
        "        \"\"\"Sets control actions\"\"\" \n",
        "        self._action = action \n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Returns physical states\"\"\"\n",
        "        return self._state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9MX4dZWJTN"
      },
      "source": [
        "### task\n",
        "Each **task** has several purposes:\n",
        "\n",
        "-  Initialize the physics.\n",
        "\n",
        "For example, initializing the initial water level. This could differ from task to task.\n",
        "\n",
        "-  Define the reward function, hence the control target\n",
        "\n",
        "In this tutorial we will consider a Step target, which aims to keep the water level constant during a first time interval and then step the level to a different constant target. The reward function is defined as a normal distribution, with the mean equal to the target water level at a given time and the standard deviation $\\sigma$ set to $0.05m$. A well trained agent should be able to control the water level with a precision $\\sim \\sigma$. \n",
        "\n",
        "-  Provide the observations to be sent to the actor given the state of the system and the control target for the task.\n",
        "\n",
        "Each task could potentially observe a different subset of the system state. In this tutorial the agent will observe directly the water level, hence the full physical state. In order to let the agent learn how to deal with a time varying target, together with the physical state, we let the system observe the water level desired target.\n",
        "\n",
        "- Define physical limits for the actions.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_GP5BVQmifv"
      },
      "outputs": [],
      "source": [
        "class Step(control.Task):\n",
        "    \"\"\" Step task:\n",
        "    Keep constant value and step to different constant value at t_step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 precision: float,\n",
        "                 targets,\n",
        "                 ):\n",
        "        \"\"\"Initialize Step task\n",
        "        \n",
        "        Parameters:\n",
        "            precision: [m] desired precision on h target   \n",
        "        \n",
        "        \"\"\"\n",
        "        self._precision = precision\n",
        "        self._targets = targets\n",
        "\n",
        "\n",
        "    def initialize_episode(self, physics):\n",
        "        \"\"\" Reset physics for the task \"\"\"\n",
        "        physics.reset()\n",
        "\n",
        "    def get_reference(self, physics):\n",
        "        \"\"\"Returns target reference\"\"\"\n",
        "        # self._targets[0] = 0.05 # Target prime_xpxp  \n",
        "        return self._targets\n",
        "\n",
        "    def get_observation(self, physics):\n",
        "        \"\"\"Returns specific observation for the task\"\"\"\n",
        "        return np.concatenate((self.get_reference(physics), physics.get_state()))\n",
        "\n",
        "    def get_reward(self, physics):\n",
        "        \"\"\"Returns the reward given the physical state \"\"\"\n",
        "        sigma = self._precision\n",
        "        mean = self.get_reference(physics)\n",
        "        rewardFunc = np.exp(-((physics.get_state()[3] - mean[0])**2.)/(2*sigma**2.))\n",
        "        return rewardFunc \n",
        "\n",
        "    def before_step(self, action, physics):\n",
        "        physics.set_control(action)\n",
        "     \n",
        "    def observation_spec(self, physics):\n",
        "        \"\"\"Returns the observation specifications\"\"\"\n",
        "        return specs.Array(\n",
        "            shape = (16,), # 15 states plus the reference, in this case the single target\n",
        "            dtype = np.float32,\n",
        "            name = 'observation')\n",
        "\n",
        "    def action_spec(self, physics):\n",
        "        \"\"\"Returns the action specifications\"\"\"\n",
        "        return specs.BoundedArray(\n",
        "            shape = (15,), # there are 15 actions, all the primes_alpha1alpha2\n",
        "            dtype = np.float32,\n",
        "            minimum = -10.,\n",
        "            maximum = 10.,\n",
        "            name = 'action')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyH8KzpaoIY"
      },
      "source": [
        "## Simulate the environment with null actions\n",
        "We can now instantiate the environment and simulate it with null actions to get some intuition of the different components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox3Fk9gg0L90"
      },
      "source": [
        "Instance of `physics`, `task` and `environment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "estuEgJoxqn_"
      },
      "outputs": [],
      "source": [
        "physics = Physics(\n",
        "    dt = 0.05,  \n",
        "    init_state = np.zeros((15,)).tolist(),  \n",
        "    init_action = np.ones((15,)).tolist(),\n",
        "    min_Rep = [0.],\n",
        "    max_Rep = [50.],\n",
        "    Re_inf = [10000.],\n",
        "    dp = [np.sqrt(18.*0.5/(10000.*250.))], # np.sqrt(18*St/(Re_inf*rhop))\n",
        "    St = 0.5,\n",
        "    )\n",
        "\n",
        "task = Step(\n",
        "    precision = 0.05, \n",
        "    targets = [0.1],\n",
        "    #targets = [0.],\n",
        ")\n",
        "\n",
        "environment = Environment(\n",
        "    physics,\n",
        "    task,\n",
        "    time_limit = 10, \n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp2piCOcbSYX"
      },
      "source": [
        "Simulate with null action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TimeStep.observation.tolist()"
      ],
      "metadata": {
        "id": "xqtpUkEnE02M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[10]"
      ],
      "metadata": {
        "id": "Xb4YOVLLF6F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz9J2rDc_NWv"
      },
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "TimeStep = environment.reset()\n",
        "    \n",
        "# Define constant 0 actions\n",
        "actions = np.zeros(environment.action_spec().shape, np.float32)\n",
        "\n",
        "# Simulate environment and store state,observation,reward,time\n",
        "s, o, r, t = [],[],[],[]\n",
        "mean_xp, mean_up, prime_xpxp, prime_xpup, prime_upup, prime_alpha1xp, prime_alpha2xp, prime_alpha3xp, prime_alpha4xp, prime_alpha5xp, prime_alpha1up, prime_alpha2up, prime_alpha3up, prime_alpha4up, prime_alpha5up = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "while not TimeStep.last():\n",
        "  s.append(environment._physics._state)\n",
        "  o.append(TimeStep.observation.tolist())\n",
        "  r.append(TimeStep.reward)\n",
        "  t.append(environment.physics.time())\n",
        "  TimeStep = environment.step(actions)\n",
        "  mean_xp.append((environment._physics._state[0]))\n",
        "  mean_up.append((environment._physics._state[1]))\n",
        "  prime_xpxp.append((environment._physics._state[2]))\n",
        "  prime_xpup.append((environment._physics._state[3]))\n",
        "  prime_upup.append((environment._physics._state[4]))\n",
        "  prime_alpha1xp.append((environment._physics._state[5]))\n",
        "  prime_alpha2xp.append((environment._physics._state[6]))\n",
        "  prime_alpha3xp.append((environment._physics._state[7]))\n",
        "  prime_alpha4xp.append((environment._physics._state[8]))\n",
        "  prime_alpha5xp.append((environment._physics._state[9]))\n",
        "  prime_alpha1up.append((environment._physics._state[10]))\n",
        "  prime_alpha2up.append((environment._physics._state[11]))\n",
        "  prime_alpha3up.append((environment._physics._state[12]))\n",
        "  prime_alpha4up.append((environment._physics._state[13]))\n",
        "  prime_alpha5up.append((environment._physics._state[14]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE1ChHGccAum"
      },
      "source": [
        "As expected the water level drops from the initial level $h = 1$ to $0$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environment._physics._state"
      ],
      "metadata": {
        "id": "2JdtA8LZDJJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[0]"
      ],
      "metadata": {
        "id": "Jl0euTfBCv9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt0a9IpDmNCr"
      },
      "outputs": [],
      "source": [
        "# Plot system state evolution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(t,mean_xp)\n",
        "plt.plot(t,mean_up)\n",
        "plt.ylabel('mean x_p, mean u_p')\n",
        "plt.xlabel('t')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(t,prime_xpxp)\n",
        "plt.plot(t,prime_xpup)\n",
        "plt.plot(t,prime_upup)\n",
        "plt.ylabel('xpxp, xpup, upup')\n",
        "plt.xlabel('t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWrpQcVBdMF2"
      },
      "source": [
        "We can check the observations in time that the task `Step` will provide to the agent. As you can see in the figure, the `target` level is provided to the observations together with the `state` to allow the agent learning to follow the target in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35c3vPeamqRq"
      },
      "outputs": [],
      "source": [
        "# Plot observation\n",
        "plt.plot(t,o)\n",
        "plt.ylabel('observations')\n",
        "plt.xlabel('time [s]')\n",
        "plt.legend(['target','state'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvByARCinayT"
      },
      "source": [
        "We can also check the time trace of the rewards provided by the task. Given the normal definition centered at the target, the reward drops while the water level gets further from the target. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nom9KzuPnQuX"
      },
      "outputs": [],
      "source": [
        "# Plot reward\n",
        "plt.plot(t,r)\n",
        "plt.ylabel('reward')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnEeXaCCqmJv"
      },
      "source": [
        "# **MPO Agent**\n",
        "\n",
        "In the following section we will show how to train an MPO agent for the environment and task. \n",
        "The section is taken mainly from the official [acme](https://github.com/deepmind/acme) tutorial in the following colab <a href=\"https://colab.research.google.com/github/deepmind/acme/blob/master/examples/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>. We invite to check the official tutorial for a more detailed explanation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrni55Yhl_j"
      },
      "outputs": [],
      "source": [
        "# Set random seed for example reproducibility\n",
        "tf.random.set_seed(1500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOOgTwTGl5S9"
      },
      "source": [
        "First of all we make use of wrappers to wrap the environment in order to bound the allowed actions within specifications defined in task. We also cast the I/O of the environment into single precision. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75LSF_k7lteP"
      },
      "outputs": [],
      "source": [
        "# Clip actions by bounds\n",
        "environment = CanonicalSpecWrapper(environment= environment,clip= True) \n",
        "\n",
        "# Wrap to single precision\n",
        "environment = SinglePrecisionWrapper(environment) \n",
        "\n",
        "# Extract environment specifications\n",
        "environment_spec = specs.make_environment_spec(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlIFQ8dm9Ld"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "The next step is to set-up the NN used by the agent. MPO is a somewhat complicated algorithm, so we'll leave a full explanation of this method to the accompanying  [paper](https://arxiv.org/abs/1806.06920). \n",
        "\n",
        "However, we give here below some hints to understand the meaning of the following code. The `mpo` is an actor-critic algorithm. \n",
        "- The `actor` provides the actions given the state (of the agent). In this simple example the state of the agent consists directly on the observations, i.e. the state of the physical system (water level) and the target level. The actor contains the policy, which ultimately represents the control low to decide the inflow given the actor state (water level).  \n",
        "- The `critic` learns the state-action value function, which is related to the expected sum of future rewards, given a certain state and a taken action. This function provides intuitively the information on the value of taking a certain action being on a certain state. The critic is used during the learning process to update the policy. \n",
        "\n",
        "The `actor` and `critic` functions are approximated with NNs as [sonnet](https://github.com/deepmind/sonnet) MLP modules.\n",
        "\n",
        "MPO uses a distributional `actor`, as it can be seen from the `MultivariateNormalDiagHead`, which means that the policy obtained is not deterministic. The `policy_network` will return the mean and standard deviation of Normal distribution and the `actor` will sample from this distribution in order to get the actions to be applied to the environment. \n",
        "\n",
        "The [acme](https://github.com/deepmind/acme) architecture provides the possibility to specify a neural network for the observations. This is useful for example when dealing with observations coming from images to distil a simple agent state from pixel like information to be given to the critic. In this tuorial, the single physical state $h$ is directly observed and the observation network is simply an identity. \n",
        "\n",
        "The `multiplex` combines the actions and observations to be given as inputs to the critic network. \n",
        "\n",
        "Overall in general, for the mpo algorithm, one needs to specify 3 NNs for the `actor(policy)_net`, the `critic_net` and  the `observation_net`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total number of action dimensions from action spec.\n",
        "num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n",
        "num_dimensions"
      ],
      "metadata": {
        "id": "_vMhbGb9gniB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the shared observation network; here simply a state-less operation.\n",
        "observation_network = tf2_utils.batch_concat\n",
        "observation_network"
      ],
      "metadata": {
        "id": "eZK3hivJlEz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify default dimension for MLP\n",
        "policy_layer_sizes = [16,16]\n",
        "critic_layer_sizes = [16,16]\n",
        "\n",
        "# Create the policy network.\n",
        "policy_network = snt.Sequential([\n",
        "  networks.LayerNormMLP(policy_layer_sizes),\n",
        "  networks.MultivariateNormalDiagHead(num_dimensions),\n",
        "])\n",
        "policy_network"
      ],
      "metadata": {
        "id": "KG_eYtDhm9-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic_layer_sizes = list(critic_layer_sizes) + [1] # Hack to conform to mpo implementation\n",
        "# Create the critic network\n",
        "critic_network = networks.CriticMultiplexer(critic_network=networks.LayerNormMLP(critic_layer_sizes))\n",
        "critic_layer_sizes\n",
        "critic_network"
      ],
      "metadata": {
        "id": "yVizRXx7nFvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pack agent networks\n",
        "agent_networks = {\n",
        "      'policy': policy_network,\n",
        "      'critic': critic_network,\n",
        "      'observation': observation_network,\n",
        "  }\n",
        "agent_networks"
      ],
      "metadata": {
        "id": "KPN3ieUhoR1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC1JEOVsixYT"
      },
      "outputs": [],
      "source": [
        "# Get total number of action dimensions from action spec.\n",
        "num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n",
        "\n",
        "# Create the shared observation network; here simply a state-less operation.\n",
        "observation_network = tf2_utils.batch_concat\n",
        "\n",
        "# Specify default dimension for MLP\n",
        "policy_layer_sizes = [16,16]\n",
        "critic_layer_sizes = [16,16]\n",
        "\n",
        "# Create the policy network.\n",
        "policy_network = snt.Sequential([\n",
        "  networks.LayerNormMLP(policy_layer_sizes),\n",
        "  networks.MultivariateNormalDiagHead(num_dimensions),\n",
        "])\n",
        "\n",
        "critic_layer_sizes = list(critic_layer_sizes) + [1] # Hack to conform to mpo implementation\n",
        "# Create the critic network\n",
        "critic_network = networks.CriticMultiplexer(critic_network=networks.LayerNormMLP(critic_layer_sizes))\n",
        "\n",
        "# Pack agent networks\n",
        "agent_networks = {\n",
        "      'policy': policy_network,\n",
        "      'critic': critic_network,\n",
        "      'observation': observation_network,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZivpE0xHfHl"
      },
      "source": [
        "Having specified the network architectures, we can finally define the agent, which combines the environment, the actor and the critic. Internally, the MPO agent contains the learner to update the policy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environment_spec"
      ],
      "metadata": {
        "id": "eGLZIfIapaa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V1vu12EnEy_"
      },
      "outputs": [],
      "source": [
        "agent = mpo.MPO(\n",
        " environment_spec = environment_spec,\n",
        " policy_network = agent_networks['policy'],\n",
        " critic_network = agent_networks['critic'],\n",
        " observation_network = agent_networks['observation'], \n",
        " batch_size = 40,\n",
        " target_policy_update_period = 5,\n",
        " target_critic_update_period = 5,\n",
        " min_replay_size = 10,\n",
        " checkpoint = False,\n",
        ")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYnEQsDgiVKt"
      },
      "source": [
        "We define a tensorboard logger to store logs during training and inspect the learning curve afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD4NjhNiibMZ"
      },
      "outputs": [],
      "source": [
        "outpath = '/content' # Destination of tensorboard log file\n",
        "logger = tf_summary.TFSummaryLogger(logdir = outpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCyKR567iprB"
      },
      "source": [
        "## Training\n",
        "Finally we can train the agent. (With 200 episodes it will take ~2 min)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0IDqHJZiuDB"
      },
      "outputs": [],
      "source": [
        "num_episodes = 200 \n",
        "\n",
        "# Run the environment loop.\n",
        "loop = EnvironmentLoop(environment, agent, logger = logger)\n",
        "\n",
        "# 350 is a good trainin\n",
        "loop.run(num_episodes=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHaTsyWqi6yy"
      },
      "source": [
        "# Visualize training logs with tensorboard\n",
        "\n",
        "It is convenient to visualize the training results in tensorboard. \n",
        "The main plot to observe is the episode return increase (~sum of the reward during an episode) over episodes, which tells how fast the agent is learning. \n",
        "\n",
        "Given that the simulation was set to last $2.5s$ with a $dt_{sim}$ of $0.05s$, we expect each episode to last $50$ time steps unless a physical limit defined in `physics` is hit, such as if the water exceed the maximum height.\n",
        "\n",
        "The maximum reward per time step is $1$, hence the maximum return per episode, given by the sum of the discounted rewards, is $\\sim50$. (discount factor is set to 0.99).\n",
        "\n",
        "We can see that the trained agent achieves an episode return of $\\sim 45$.\n",
        "\n",
        "Another interesting plots is the `StepsPerSecond` which tells how fast is the simulation of a single physical time step. Improving the speed of the environment allows to accelerate the collection of the experience for the learning process. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDwjsd7DBM6L"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sc0QiLey9D"
      },
      "source": [
        "# Trained agent evaluation\n",
        "\n",
        "Now that we trained an agent and inspected its learning properties, we can use the policy obtained and evaluate how it performs in controlling the water level in the `tank`. \n",
        "\n",
        "First we simulate the environment with the trained policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqyFpeqAuHf6"
      },
      "outputs": [],
      "source": [
        "# Evaluate \n",
        "TimeStep = environment.reset()\n",
        "# Run episode and store system state, observation, action, reward and time\n",
        "s, o, a, r, t = [],[],[],[],[]\n",
        "while not TimeStep.last():\n",
        "  s.append(environment._physics._state)\n",
        "  o.append(TimeStep.observation.tolist())\n",
        "  r.append(TimeStep.reward)\n",
        "  t.append(environment.physics.time())\n",
        "  actions = agent.select_action(np.float32(TimeStep.observation)) \n",
        "  a.append(actions)\n",
        "  TimeStep = environment.step(actions)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkCcc628WiCA"
      },
      "source": [
        "We can now observe how the agent has learnt to control the $w_{in}$ to follow the target water level desired. We recall that the reward was designed as a Normal distribution centered on the target, and with $\\sigma = 0.05 m$. The trained policy achieved a tracking precision of $\\sim  0.05 m$ as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSXCEwMFNRWA"
      },
      "outputs": [],
      "source": [
        "# Plot observation\n",
        "plt.plot(t,o)\n",
        "plt.ylabel('observations')\n",
        "plt.xlabel('time [s]')\n",
        "plt.legend(['target','state'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS0DgJGcXqB5"
      },
      "source": [
        "The instantaneous rewards during the simulation with the trained agent are always close to the maximum value $=1$, except during the step instant. In this moment, the time decay of the water level is limited by the physical time scales of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV-mpMLpNgRS"
      },
      "outputs": [],
      "source": [
        "# Plot reward\n",
        "plt.plot(t,r)\n",
        "plt.ylabel('reward')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68qkQW_qYQ0B"
      },
      "source": [
        "We can also investigate the actions produced by the policy. Since we used the `CanonicalSpecWrapper` to clip the actions by physical bounds specified in task, the trained policy will provide actions in the $[-1,1]$ canonical interval. We need therefore to transform the actions to convert the inflow in SI units in the $[0,maxinflow]$ interval. This convertion is done internally when using `EnvironmentLoop` during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8US021bPNmEJ"
      },
      "outputs": [],
      "source": [
        "# Revert action from canonical representation [-1,1] to SI \n",
        "f = lambda x: (np.clip(np.asarray(x),-1,1) + 1)*task._maxinflow/2 \n",
        "a = [f(x) for x in a]\n",
        "\n",
        "# Plot reward\n",
        "plt.plot(t,a)\n",
        "plt.ylabel('w_in')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMu8vIDtZYR0"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this tutorial we showed how to implement a simple physical ode environment  following the [dm_control](https://github.com/deepmind/dm_control) requirements. Then, we trained an mpo agent with [acme](https://github.com/deepmind/acme) framework to perform continuous action space control.\n",
        "\n",
        "Using deep reinforcement learning for this simple task and environment is obviously an overkill. However an extremely simple environment allows to play with deep reinforcement learning solutions at low computational costs. On top of that, making use of high quality frameworks such as [acme](https://github.com/deepmind/acme) allows potentially to easily scale up and generalize the approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "3CYYsHV8_OYv",
        "IMu8vIDtZYR0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}